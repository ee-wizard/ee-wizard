---
date: 2025-05-26
title: Paper-MPAD-A New Dimension-Reduction Method for Preserving Nearest Neighbors in High-Dimensional Vector Search
category: scholar
tags:
- scholar
- vector_search
description: è®ºæ–‡-é«˜ç»´ç©ºé—´é™ç»´æ–¹æ³•
---

# è®ºæ–‡ä¿¡æ¯

[MPAD: A New Dimension-Reduction Method for Preserving Nearest Neighbors in High-Dimensional Vector Searchï¼ˆ2025ï¼‰](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eRaZHewAAAAJ&sortby=pubdate&citation_for_view=eRaZHewAAAAJ:jU7OWUQzBzMC)

Dimension Reduction(DR)

# å†…å®¹æ‘˜è¦

## åŒç±»DRæ–¹æ³•

> We present MPADâ€”Maximum Pairwise Absolute Difference, an unsupervised DR method that explicitly preserves approximate NN relations by maximizing the margin between ğ‘˜-NNs and non-ğ‘˜-NNs under **a soft orthogonality constraint**. 

åŸºäºè½¯æ­£äº¤çº¦æŸ

> Unlike traditional DR techniques that aim to preserve global variance or pairwise distances, MPAD explicitly targets approximate nearest neighbor (ANN) fidelity. It formulates an unsupervised optimization objective that **prioritizes the relative ordering of each vectorâ€™s true ğ‘˜-nearest neighbors over its non-neighbors in the low-dimensional space**. By encouraging margin-based separation between neighbors and distractors, MPAD ensures that ANN-sensitive geometry is preserved during projection.



### Principal Component Analysis(PCA)

> projecting high-dimensional data onto a subspace spanned by the top ğ‘š principal components

> Despite its widespread use, PCA is inherently a global method: it optimizes for directions that explain the largest variance across the entire dataset, without regard to local geometry or neighborhood preservation. This characteristic makes PCA ill-suited for tasks that rely on maintaining ğ‘˜-nearest neighbor (k-NN) relationsâ€”such as approximate nearest neighbor (ANN) search and information retrievalâ€”since small distances between semantically similar points can be distorted during projection. In such contexts, **preserving local topology is often more important than capturing dominant global trends, which PCA overlooks.** (ä¿æŒå±€éƒ¨æ‹“æ‰‘ç»“æ„é€šå¸¸æ¯”æ•æ‰ PCA æ‰€å¿½ç•¥çš„ä¸»å¯¼å…¨å±€è¶‹åŠ¿æ›´ä¸ºé‡è¦ã€‚)

PCAæœ¬è´¨æ˜¯æ‰¾åˆ°æ•´ä¸ªæ•°æ®é›†çš„æœ€å¤§æ–¹å·®æ–¹å‘ï¼Œè€Œä¸è€ƒè™‘å±€éƒ¨é›†åˆå’Œé‚»åŸŸä¿æŒï¼Œå› æ­¤ä¸åˆ©äºç»´æŠ¤kè¿‘é‚»å…³ç³»

for that è¯­ä¹‰ç›¸ä¼¼ç‚¹ä¹‹é—´çš„å°è·ç¦»å¯èƒ½ä¼šåœ¨æŠ•å½±è¿‡ç¨‹ä¸­æ‰­æ›²

### Kernel PCA(K-PCA)

å¯¹PCAçš„æ‰©å±•ï¼Œé€šè¿‡éçº¿æ€§æ ¸å‡½æ•°å°†è¾“å…¥æ•°æ®æ˜ å°„åˆ°é«˜ç»´ç‰¹å¾ç©ºé—´ï¼Œç„¶åå†å˜æ¢åçš„ç©ºé—´ä¸­æ‰§è¡Œçº¿æ€§PCA

> This approach enables the discovery of non-linear patterns and manifold structures in the original data
> that standard PCAâ€”restricted to linear projectionsâ€”cannot capture.

èƒ½å¤Ÿæ•æ‰åŸå§‹æ•°æ®ä¸­çš„éçº¿æ€§æ¨¡å¼æ ¸æµå¼ç»“æ„

å¸¸ç”¨æ ¸å‡½æ•°æœ‰Gaussian (RBF) and polynomial kernels

ç¼ºç‚¹ï¼šè®¡ç®—æˆæœ¬å¤§ï¼Œéœ€è¦æ„å»ºN*Nçš„æ ¸çŸ©é˜µï¼Œç„¶åç‰¹å¾å€¼åˆ†è§£ï¼Œä¸é€‚ç”¨äºå¤§è§„æ¨¡æ•°æ®é›†

### Multidimensional Scaling(MDS)

å°†æ•°æ®ç‚¹æŠ•å½±åˆ°èƒ½å¤Ÿä¿æŒè·ç¦»çš„ä½ç»´ç©ºé—´

ç¼ºç‚¹ï¼šè®¡ç®—æˆæœ¬å¤§

https://zhuanlan.zhihu.com/p/50715681

https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html



### Random projections(RP)

> Their data-independence implies that the projection does not adapt to the structure or distribution of the input space, which can lead to suboptimal preservation of fine-grained geometric relationshipsâ€”especially
> in datasets with strong clustering, anisotropic features, or manifold structure [7 ]. In particular, while inter-point distances may be roughly maintained, the relative neighborhood rankingsâ€”critical for retrieval performanceâ€”can be significantly distorted. 

å°½ç®¡éšæœºæŠ•å½±ç®€å•ä¸”å…·æœ‰ç†è®ºä¿è¯ï¼Œä½†åœ¨åº”ç”¨äºè¿‘ä¼¼æœ€è¿‘é‚» (ANN) æœç´¢ç­‰ä»»åŠ¡æ—¶ï¼Œä»å­˜åœ¨ä¸€äº›å®é™…é™åˆ¶ã€‚å…¶æ•°æ®ç‹¬ç«‹æ€§æ„å‘³ç€æŠ•å½±æ— æ³•é€‚åº”è¾“å…¥ç©ºé—´çš„ç»“æ„æˆ–åˆ†å¸ƒï¼Œè¿™å¯èƒ½å¯¼è‡´ç»†ç²’åº¦å‡ ä½•å…³ç³»çš„ä¿å­˜æ•ˆæœä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰å¼ºèšç±»ã€å„å‘å¼‚æ€§ç‰¹å¾æˆ–æµå½¢ç»“æ„çš„æ•°æ®é›†ä¸­ [7]ã€‚å…·ä½“è€Œè¨€ï¼Œè™½ç„¶ç‚¹é—´è·ç¦»å¯ä»¥å¤§è‡´ä¿æŒï¼Œä½†ç›¸å¯¹é‚»åŸŸæ’åï¼ˆå¯¹æ£€ç´¢æ€§èƒ½è‡³å…³é‡è¦ï¼‰å¯èƒ½ä¼šå‘ç”Ÿæ˜¾è‘—æ‰­æ›²ã€‚



## Maximum Pairwise Absolute Difference (MPAD)

å®šä¹‰$R^n -> R^m$çš„æ˜ å°„ï¼Œæœ€å¤§åŒ–æ ‡é‡æŠ•å½±ä¸­çš„æœ€å°æˆå¯¹å·®å¼‚ï¼ŒåŒæ—¶é€šè¿‡è½¯æ­£äº¤æƒ©ç½šå‡å°æŠ•å½±æ–¹å‘çš„å†—ä½™

> The core algorithm iteratively selects ğ‘š projection directions by optimizing a signed objective function ğœ™ that balances informativeness and orthogonality. 

æ ¸å¿ƒç®—æ³•é€šè¿‡ä¼˜åŒ–å¹³è¡¡ä¿¡æ¯é‡å’Œæ­£äº¤æ€§çš„æœ‰ç¬¦å·ç›®æ ‡å‡½æ•°ğœ™æ¥è¿­ä»£é€‰æ‹©ğ‘šæŠ•å½±æ–¹å‘

å’ŒPCAä¸€æ ·çš„æ€è·¯ï¼Œåœ¨å¤šä¸ªç»´åº¦ä¸­ä¼˜å…ˆé€‰æ‹©æ–¹å·®æœ€å¤§çš„ç»´åº¦ï¼Œä½†æ˜¯==PCAé¢ä¸´ä¸‰ä¸ªé—®é¢˜==

1. **Infomation trade-off**ã€‚å³ä¼šæŸå¤±ç»†èŠ‚
2. **Local versus Global Structure**
3. **Emphasis on Pairwise Differences**

> In light of these insights, our method selects the dimension that maximizes the average of the smallest ğ‘% of the pairwise absolute differences.

==é«˜ç»´ç©ºé—´çš„ç‰¹æ€§==

1. **Concentration of Measure**

> In high dimensions, pairwise distances tend to become nearly uniform, complicating the differentiation between close and distant points.
>
> åœ¨é«˜ç»´åº¦ä¸­ï¼Œæˆå¯¹è·ç¦»è¶‹äºå˜å¾—å‡ ä¹å‡åŒ€ï¼Œè¿™ä½¿å¾—è¿‘ç‚¹å’Œè¿œç‚¹ä¹‹é—´çš„åŒºåˆ†å˜å¾—å¤æ‚ã€‚

2. **Redundancy**

> Typically, only a few dimensions contain most of the discriminative information, while the remainder may contribute noise or redundancy.

3. **Dominance of Local Structure**

> The intrinsic organization of the data is frequently governed by local relationships rather than by global variance, **making it imperative to preserve the finer details of the nearest neighbor topology.**



### Algorithm

1. éšæœºç”Ÿæˆ1ä¸ªnç»´å•ä½å‘é‡w1ä½œä¸ºåˆå§‹æŠ•å½±å‘é‡

$$
f_{w_1}(x_i \in X) = proj_{w_1}(x_i) = <x_i,w_1> \cdot w_1 \\
$$

$$
d_{1,ij} = ||f_{w_1}(x_i)-f_{w_1}(x_j)|| \\
$$

$$
D_{1,b} = \{ d_{1,ij} | Smallest \ b\% \ of \ d_{1,ij} \} \\
$$

$$
\mu_{b}(w_1) = \frac{1}{|D_{1,b}|}\sum_{d \in D_{1,b}} d \\
$$

$$
argmax_{w_1}{\mu_{b}(w_1)} = armax_{w_1}{\frac{1}{|D_{1,b}|}\sum_{d \in D_{1,b}} d}
$$

2. å¯¹äºç¬¬2ä¸ªåŸºå‘é‡w2ï¼Œå¯¹å…¶æ–½åŠ ä¸€ä¸ªæ­£äº¤æ€§æƒ©ç½š**orthogonality penalty**

è¿™é‡Œä¸PCAä¸åŒï¼Œå¹¶ä¸å¼ºåˆ¶æ­£äº¤

> This is because we believe that in many real-life scenarios some directions indeed convey more information than others, and there is no reason to force them to be orthogonal. 

$$
P_{orth,2} = \alpha (w_1 \cdot w_2) ^2
$$

Generalized For
$$
P_{orth,k} = \alpha \sum_{i=1}^{k-1} (w_i \cdot w_k)^2
$$
ç„¶åä½¿å¾—
$$
argmax_{w_2} {(\mu_b (w_2) - P_{orth,2}})
$$
ç»è¿‡Tæ¬¡è¿­ä»£ï¼Œæœ€ç»ˆæ„é€ ä¸€ä¸ªæŠ•å½±å‘é‡$w_k$